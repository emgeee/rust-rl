# Server configuration
server:
  host: "localhost"
  port: 8000
  runtime:
    gpu_memory_utilization: 0.9
    enable_chunked_prefill: true
    tensor_parallel_size: 1

# All models configuration
models:
  api:
    - model: "claude-3-5-sonnet-20241022"
      provider: "anthropic"

    - model: "claude-3-7-sonnet-20250219"
      provider: "anthropic"

    # - model: "claude-opus-4-20250514"
    #   provider: "anthropic"

    - model: "claude-sonnet-4-20250514"
      provider: "anthropic"

    - model: "gpt-4o"
      provider: "openai"

    - model: "o4-mini"
      provider: "openai"

    - model: "gpt-4.1"
      provider: "openai"

    - model: "gpt-4.1-mini"
      provider: "openai"

    - model: "gpt-4.1-nano"
      provider: "openai"

    - model: "grok-3"
      provider: "xai"

    - model: "grok-3-mini"
      provider: "xai"

    # - model: "gemini-1.5-flash"
    #   provider: "google"
    #
    # - model: "gemini-1.5-pro"
    #   provider: "google"
    #
    # - model: "gemini-2.0-flash"
    #   provider: "google"
    #
    # - model: "gemini-2.5-flash-preview-05-20"
    #   provider: "google"
    #
    # - model: "gemini-2.5-pro-preview-05-06"
    #   provider: "google"

  vllm:
    # These models will be served by the vLLM server and used by client
    # - model: "mgreen/QWEN-2.5-1.5B-instruct-rust-ft-8800"
    #   max_model_len: 32768

    - model: "Qwen/Qwen2.5-Coder-7B-Instruct"
      max_model_len: 32768

    # - model: "deepseek-ai/deepseek-coder-6.7b-instruct"
    #   max_model_len: 16384
    #
    # - model: "Qwen/Qwen2.5-Coder-1.5B-Instruct"
    #   max_model_len: 32768

generation_params:
  max_new_tokens: 2048
  temperature: 0.2
  top_p: 0.9

dataset:
  path: "qwen3-rust-finetune/cargo_test_passed_eval.parquet"
  eval_rows: Null  # Number of rows to use from eval dataset. If null, use all rows

output:
  base_dir: "qwen3-rust-finetune/outputs"

evaluation:
  tools: ["build", "clippy", "test"]
  save_every: 10

